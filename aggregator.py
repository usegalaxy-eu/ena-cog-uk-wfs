import argparse
import csv
import os
import re
import sys

from math import exp


def poissonSupport(MLL, support, N):
  el = exp(-MLL)
  lkf = 1
  points = [k for k in range(len(support)+1)]
  i = 0
  sp = []
  for p in points:
      sp.append(el*lkf*N)
      i += 1
      lkf *= MLL/i

  return sp


if __name__ == '__main__':
    arguments = argparse.ArgumentParser(
        description='Aggregate Galaxy-generated by-sample variant report '
                    'files into condensed variant tables for consumption by '
                    'Observable Dashboards. Also generate diagnostic Poisson '
                    'statistics for each batch of data/input file.'
    )
    arguments.add_argument(
        'input_folder',
        help='Folder with by-sample variant reports to process'
    )
    arguments.add_argument(
        '-c', '--columns', required=True, nargs='*',
        help='Columns to extract'
    )
    arguments.add_argument(
        '-b', '--add-batch-id', action='store_true',
        help='Add an additional column to the output representing the '
             'analysis batch ID derived from each input filename'
    )
    arguments.add_argument(
        '-d', '--diagnostics', type=argparse.FileType('w'),
        help='Output batch-level diagnostics to this TSV file'
    )
    arguments.add_argument(
        '-P', '--position-field', default='POS',
        help='The name of variant table column which contains genomic variant '
             'position'
    )
    arguments.add_argument(
        '-A', '--af-field', default='AF',
        help='The name of the variant table column which contains AF'
    )
    arguments.add_argument(
        '-f', '--freq', type=float, default=0.25,
        help='Maximim AF to consider for Poisson filtering'
    )
    arguments.add_argument(
        '--headers', action='store_true',
        help='Write a header line to both the diagnostics and the main '
             'variants output'
    )
    arguments.add_argument(
        '--genome-length', type=int, default=29903,
        help='Length of the reference genome that variants have been called '
             'against (default: 29903); required for calculation of '
             'diagnostic stats'
    )
    import_settings = arguments.parse_args()

    if import_settings.diagnostics:
        # set up Poisson diagnostic stats output
        genome_length = import_settings.genome_length
        diag_annotation = csv.writer(
            import_settings.diagnostics, delimiter='\t'
        )
    if import_settings.headers:
        diag_annotation.writerow(['batch', 'N', 'error_rate', 'poisson_cut'])

    # set up main variants output
    writer = csv.writer(sys.stdout, delimiter='\t', lineterminator='\n')
    keep_in = {}
    headers = []
    shift = 0
    for i, n in enumerate(import_settings.columns):
        pieces = n.split('=')
        if len(pieces) == 1:
            keep_in[n] = (i - shift, )
            headers.append(n)
        else:
            keep_in[pieces[0]] = (i, pieces[1])
            shift += 1
    if import_settings.headers:
        writer.writerow(headers)

    # read input by-sample variant reports from folder
    with os.scandir(import_settings.input_folder) as files_it:
        for entry in files_it:
            if entry.is_file():
                # count how often a position has mutations
                hit_count = {}
                # keep track of sample names
                samples = set()
                # if requested, extract first hexadecimal part of file name
                # and store it for adiing it as the batch ID to rows
                if import_settings.add_batch_id:
                    try:
                        extra_cols = [re.match('[a-f0-9]+', entry.name).group(0)]
                    except AttributeError:
                        sys.exit(
                            'Failed to extract batch ID from file name: {0}'
                            .format(entry.name)
                        )
                else:
                    extra_cols = []
                with open(entry.path) as itsv:
                    reader = csv.reader(itsv, delimiter='\t')
                    names = next(reader)
                    pos_idx = names.index(import_settings.position_field)
                    af_idx = names.index(import_settings.af_field)
                    field_mapper = {}
                    field_filter = []
                    for i, n in enumerate(names):
                        if n in keep_in:
                            if len(keep_in[n]) == 1:
                                field_mapper[i] = keep_in[n][0]
                            else:
                                field_filter.append([i, keep_in[n][1]])

                    for row in reader:
                        samples.add(row[0])
                        new_row = ['' for k in field_mapper]
                        for i, o in field_mapper.items():
                            new_row[o] = row[i]
                        if len([k for k in field_filter if row[k[0]] != k[1]]) > 0:
                            continue

                        if float(row[af_idx]) <= import_settings.freq:
                            pos = int(row[pos_idx])
                            if pos in hit_count:
                                hit_count[pos] += 1
                            else:
                                hit_count[pos] = 1
                        new_row += extra_cols
                        writer.writerow(new_row)

                if import_settings.diagnostics:
                    # write Poisson stats for this file
                    # convert position=>hits into hits=># of positions
                    N = len(samples)
                    diag_row = [entry.name.split('_')[0], str(N), 'N/A', 'N/A']
                    if N >= 10:
                        max_hits = max(
                            hit_count.items(),
                            key=lambda x: x[1]
                        )[1]
                        hit_array = [0 for k in range(max_hits+1)]
                        hit_array[0] = genome_length - len(hit_count)
                        for p, v in hit_count.items():
                            hit_array[v] += 1
                        ml_lambda = sum(
                            [i * k for i, k in enumerate(hit_array)]
                        ) / genome_length
                        expected_counts = poissonSupport(
                            ml_lambda, hit_array, genome_length
                        )
                        cutoff = 1
                        for i, k in enumerate(hit_array):
                            if i > len(expected_counts): break
                            if k > 0:
                                if (
                                    expected_counts[i] - k
                                ) / expected_counts[i] < -1:
                                    break
                                cutoff += 1

                        diag_row[2] = str(ml_lambda / genome_length)
                        diag_row[3] = str(cutoff)

                        #print(expected_counts, file=sys.stderr)
                        #print(hit_array, file=sys.stderr)

                        print(
                            'ID %s: Poisson cutoff %d, error rate %g'
                            % (
                              entry.name.split('_')[0],
                              cutoff,
                              ml_lambda / genome_length
                            ),
                            file=sys.stderr
                        )

                    diag_annotation.writerow(diag_row)
